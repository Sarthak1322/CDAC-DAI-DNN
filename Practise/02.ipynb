{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a151a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a simple CNN (classification) model for the CIFAR-10 Dataset using TensorFlow or PyTorch**\n",
    "\n",
    "### **Tasks:**\n",
    "#### **Data Preparation**\n",
    "1. Load the CIFAR-10 dataset: Download and load the train and test data\n",
    "2. Preprocess the data: Normalize the input features, apply data augmentation techniques, and split the dataset into training and testing sets\n",
    "\n",
    "#### **Model Architecture**\n",
    "1. Design a Convolutional Neural Network (CNN) architecture\n",
    "2. Define the number of convolutional layers, filter sizes, activation functions, and pooling layers\n",
    "3. Add fully connected layers after feature extraction\n",
    "\n",
    "#### **Model Training**\n",
    "1. Initialize the model parameters (weights and biases)\n",
    "2. Define a loss function appropriate for classification (e.g., cross-entropy)\n",
    "3. Train the model using stochastic gradient descent or any suitable optimization algorithm\n",
    "4. Monitor the training process by observing the loss and accuracy metrics\n",
    "\n",
    "#### **Model Evaluation**\n",
    "1. Evaluate the trained model on the testing dataset\n",
    "2. Calculate and report the accuracy\n",
    "\n",
    "#### **Experimentation**\n",
    "1. Experiment with different hyperparameters such as learning rate, number of convolutional layers, and filter sizes\n",
    "2. Explore different activation functions (e.g., ReLU, sigmoid, tanh) and observe their impact on the model's performance\n",
    "3. Test different pooling strategies (Max pooling, Average pooling) to analyze their effects\n",
    "\n",
    "#### **Deliverables**\n",
    "1. Python code (.ipynb) implementing the CNN\n",
    "2. Training loss curve plot\n",
    "3. Evaluation results saved in `predictions.csv` file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e436de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a Simple CNN (Classification) Model for a Flower Classification Dataset using TensorFlow or PyTorch\n",
    "üîß Tasks:\n",
    "üìÅ Data Preparation\n",
    "Load the Flower Dataset\n",
    "\n",
    "Download and load the train and test data from a flower image classification dataset (e.g., TensorFlow Datasets: tfds.load(\"tf_flowers\"), or custom flower dataset with labeled images).\n",
    "\n",
    "Preprocess the Data\n",
    "\n",
    "Normalize the input features (pixel values to range [0, 1] or [-1, 1])\n",
    "\n",
    "Apply data augmentation techniques such as random flip, rotation, and zoom\n",
    "\n",
    "Resize all images to a uniform shape (e.g., 128x128 or 224x224)\n",
    "\n",
    "Split the dataset into training and testing sets (e.g., 80/20 split)\n",
    "\n",
    "üß† Model Architecture\n",
    "Design a Convolutional Neural Network (CNN)\n",
    "\n",
    "Stack convolutional layers with filters (e.g., 32, 64, 128)\n",
    "\n",
    "Use activation functions (ReLU, sigmoid, or tanh)\n",
    "\n",
    "Add pooling layers (MaxPooling or AveragePooling) after convolutions\n",
    "\n",
    "Add Fully Connected Layers\n",
    "\n",
    "Flatten the output from convolutional layers\n",
    "\n",
    "Add one or more Dense layers\n",
    "\n",
    "Final output layer should match the number of flower categories with softmax activation\n",
    "\n",
    "üéØ Model Training\n",
    "Initialize the Model Parameters\n",
    "\n",
    "Set initial weights using default or specific initialization strategies\n",
    "\n",
    "Define the Loss Function\n",
    "\n",
    "Use categorical_crossentropy (for one-hot labels) or sparse_categorical_crossentropy (for integer labels)\n",
    "\n",
    "Train the Model\n",
    "\n",
    "Use an optimizer like SGD, Adam, or RMSprop\n",
    "\n",
    "Set batch size, number of epochs, and learning rate\n",
    "\n",
    "Track loss and accuracy during training\n",
    "\n",
    "Visualize the Training\n",
    "\n",
    "Plot the training and validation loss and accuracy curves\n",
    "\n",
    "üß™ Model Evaluation\n",
    "Evaluate the Model on the Test Set\n",
    "\n",
    "Report accuracy and optionally show a confusion matrix\n",
    "\n",
    "Predict on test data and generate a predictions.csv file with image ID and predicted class\n",
    "\n",
    "üß¨ Experimentation\n",
    "Hyperparameter Tuning\n",
    "\n",
    "Try different learning rates, batch sizes, and number of convolutional layers\n",
    "\n",
    "Change the number and size of filters\n",
    "\n",
    "Try Different Activation Functions\n",
    "\n",
    "Compare ReLU, tanh, and sigmoid on model performance\n",
    "\n",
    "Compare Pooling Strategies\n",
    "\n",
    "Test MaxPooling2D vs AveragePooling2D and compare accuracy and training time\n",
    "\n",
    "üì¶ Deliverables\n",
    "Python Code Notebook (.ipynb) implementing the full CNN pipeline for flower classification\n",
    "\n",
    "Training Loss Curve\n",
    "\n",
    "Plot showing loss and accuracy over epochs\n",
    "\n",
    "Evaluation Results\n",
    "\n",
    "A file named predictions.csv with predicted labels on test data (format: image_id, predicted_class)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5165e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93912968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preparation\n",
    "\n",
    "Load the Flower Dataset\n",
    "\n",
    "Download and load the train and test data from a flower image classification dataset (e.g., TensorFlow Datasets: tfds.load(\"tf_flowers\"), or custom flower dataset with labeled images).\n",
    "\n",
    "Preprocess the Data\n",
    "\n",
    "Normalize the input features (pixel values to range [0, 1] or [-1, 1])\n",
    "\n",
    "Apply data augmentation techniques such as random flip, rotation, and zoom\n",
    "\n",
    "Resize all images to a uniform shape (e.g., 256x256)\n",
    "\n",
    "Split the dataset into training and testing sets (e.g., 80/20 split)\n",
    "\"\"\"\n",
    "\n",
    "X= pd.read_csv(\"train.csv\")\n",
    "y = pd.read_csv(\"val.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26783d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13642 files belonging to 14 classes.\n",
      "Found 98 files belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "#we will normalize it to 0 to 1.\n",
    "\n",
    "#generators\n",
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory = r'/home/sarthakredasani/Documents/CDAC_DNN/Practise/train',\n",
    "    labels='inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size=32,\n",
    "    image_size=(256,256)\n",
    ")\n",
    "\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory = r'/home/sarthakredasani/Documents/CDAC_DNN/Practise/val',\n",
    "    labels='inferred',\n",
    "    label_mode = 'int',\n",
    "    batch_size=32,\n",
    "    image_size=(256,256)\n",
    ")\n",
    "def process(image,label):\n",
    "  image = tf.cast(image ,tf.float32)/255\n",
    "  return image,label\n",
    "\n",
    "train_ds = train_ds.map(process)\n",
    "validation_ds = validation_ds.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c07928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Data augmentation pipeline (used inside the model or separately)\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomTranslation(0.2, 0.2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e13bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 17:31:29.511127: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25165824 exceeds 10% of free system memory.\n",
      "2025-06-01 17:31:29.635070: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25165824 exceeds 10% of free system memory.\n",
      "2025-06-01 17:31:29.706006: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25165824 exceeds 10% of free system memory.\n",
      "2025-06-01 17:31:29.852549: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25165824 exceeds 10% of free system memory.\n",
      "2025-06-01 17:31:29.909502: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25165824 exceeds 10% of free system memory.\n",
      "2025-06-01 17:31:39.198858: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 9 of 1000\n",
      "2025-06-01 17:31:50.839929: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 20 of 1000\n",
      "2025-06-01 17:32:10.525011: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 28 of 1000\n",
      "2025-06-01 17:32:20.612448: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 32 of 1000\n",
      "2025-06-01 17:32:30.772724: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 36 of 1000\n",
      "2025-06-01 17:32:50.462559: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 44 of 1000\n",
      "2025-06-01 17:33:10.277658: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 52 of 1000\n",
      "2025-06-01 17:33:20.910927: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 56 of 1000\n",
      "2025-06-01 17:33:31.600833: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:34: Filling up shuffle buffer (this may take a while): 60 of 1000\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create folder to save images\n",
    "save_dir = './augmented_samples'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Get one batch from the augmented training dataset\n",
    "for images, labels in train_ds.take(1):  # Take one batch\n",
    "    for i in range(5):  # Save first 5 images\n",
    "        img = images[i].numpy()\n",
    "        label = labels[i].numpy()\n",
    "        \n",
    "        # Convert from float32 to uint8 for saving\n",
    "        img = (img * 255).astype('uint8')\n",
    "        \n",
    "        # Save using matplotlib\n",
    "        plt.imsave(f\"{save_dir}/augmented_image_{i}_label_{label}.png\", img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b31c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
